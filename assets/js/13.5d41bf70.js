(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{351:function(s,t,a){s.exports=a.p+"assets/img/dbscan1.a5f9aee1.png"},385:function(s,t,a){"use strict";a.r(t);var n=a(42),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,n=s._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[n("h1",{attrs:{id:"dbscan-密度聚类算法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#dbscan-密度聚类算法"}},[s._v("#")]),s._v(" DBSCAN 密度聚类算法")]),s._v(" "),n("h2",{attrs:{id:"_1-dbscan原理"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-dbscan原理"}},[s._v("#")]),s._v(" 1. DBSCAN原理")]),s._v(" "),n("ol",[n("li",[s._v("DBSCAN(Density-Based Spatial Clustering of Applications with Noise, 具有噪声的基于密度的聚类方法)")]),s._v(" "),n("li",[s._v("密度聚类原理: 通过将紧密相连的样本划分为一类, 这样就得到了一个聚类类别. 通过将所有各组紧密想来你的样本划为各个不同的类别, 就得到的最终的聚类结果")]),s._v(" "),n("li",[s._v("优点\n"),n("ol",[n("li",[s._v("不需要设置分类个数")]),s._v(" "),n("li",[s._v("适用于任意形状数据")]),s._v(" "),n("li",[s._v("对异常点数据不敏感")]),s._v(" "),n("li",[s._v("聚类结果没有偏倚")])])]),s._v(" "),n("li",[s._v("缺点\n"),n("ol",[n("li",[s._v("样本密度不均匀, 聚类间距相差很大时, 聚类效果较差")]),s._v(" "),n("li",[s._v("样本集较大时, 聚类收敛时间较长")]),s._v(" "),n("li",[s._v("调参相对复杂")])])])]),s._v(" "),n("h2",{attrs:{id:"_2-源码"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-源码"}},[s._v("#")]),s._v(" 2. 源码")]),s._v(" "),n("ol",[n("li",[n("p",[s._v("基于sklearn的DBSCAN")]),s._v(" "),n("div",{staticClass:"language-python line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("datasets "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" make_circles"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" make_blobs\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" matplotlib"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("pyplot "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" plt\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cluster "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" DBSCAN\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" np\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 创建数据集")]),s._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 环形分布数据: 样本点数, 内圈外圈距离之比, 噪声点的标准差")]),s._v("\nx1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y1 "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" make_circles"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("n_samples"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("5000")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" factor"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v(".6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" noise"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v(".05")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 团状分布数据: 样本点数, 数据维度, 中心点, 标准差")]),s._v("\nx2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" make_blobs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("n_samples"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("1000")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" n_features"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" centers"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" cluster_std"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v(".1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 数组拼接")]),s._v("\nx "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("concatenate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" x2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# eps: 邻域的距离阈值, 默认0.5")]),s._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# min_samples: 领域的样本数阈值, 默认值5")]),s._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# metric: 最近邻距离度量参数, 默认欧氏距离")]),s._v("\ny_pred "),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" DBSCAN"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("eps"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit_predict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("scatter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" c"),n("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("y_pred"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nplt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("show"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n")])]),s._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[s._v("1")]),n("br"),n("span",{staticClass:"line-number"},[s._v("2")]),n("br"),n("span",{staticClass:"line-number"},[s._v("3")]),n("br"),n("span",{staticClass:"line-number"},[s._v("4")]),n("br"),n("span",{staticClass:"line-number"},[s._v("5")]),n("br"),n("span",{staticClass:"line-number"},[s._v("6")]),n("br"),n("span",{staticClass:"line-number"},[s._v("7")]),n("br"),n("span",{staticClass:"line-number"},[s._v("8")]),n("br"),n("span",{staticClass:"line-number"},[s._v("9")]),n("br"),n("span",{staticClass:"line-number"},[s._v("10")]),n("br"),n("span",{staticClass:"line-number"},[s._v("11")]),n("br"),n("span",{staticClass:"line-number"},[s._v("12")]),n("br"),n("span",{staticClass:"line-number"},[s._v("13")]),n("br"),n("span",{staticClass:"line-number"},[s._v("14")]),n("br"),n("span",{staticClass:"line-number"},[s._v("15")]),n("br"),n("span",{staticClass:"line-number"},[s._v("16")]),n("br"),n("span",{staticClass:"line-number"},[s._v("17")]),n("br"),n("span",{staticClass:"line-number"},[s._v("18")]),n("br"),n("span",{staticClass:"line-number"},[s._v("19")]),n("br"),n("span",{staticClass:"line-number"},[s._v("20")]),n("br"),n("span",{staticClass:"line-number"},[s._v("21")]),n("br"),n("span",{staticClass:"line-number"},[s._v("22")]),n("br")])])]),s._v(" "),n("li",[n("p",[s._v("效果")]),s._v(" "),n("p",[n("img",{attrs:{src:a(351),alt:"dbscan1"}})])])])])}),[],!1,null,null,null);t.default=e.exports}}]);